# coding=utf-8import numpy as npimport scipy.sparse as spimport torchimport randomfrom sklearn.manifold import TSNEimport matplotlib.pyplot as pltimport mathimport scipy.io as siofrom skimage.segmentation import slic,mark_boundariesfrom sklearn.cluster import KMeansfrom skimage import iofrom EProjSimplex_new import EProjSimplex_newfrom ind2sub import ind2sub# import utils.hypergraph_utils as hgut# import os# import pdb# from sklearn.decomposition import PCAimport h5pydef load_data():    """Load """    matfn = "E:\github\GCGCN_code\datasets\Salinas_gyh.mat"    data_all = sio.loadmat(matfn)['Salinas_gyh']    matlabel = "E:\github\GCGCN_code\datasets\Salinas_gt.mat"    gnd = sio.loadmat(matlabel)['Salinas_gt']    gnd = np.array(gnd)    gnd = np.transpose(gnd)     gnd = np.array(gnd)    gt = gnd    data_all = np.array(data_all)    data_all = (data_all-np.min(data_all)) / (np.max(data_all)-np.min(data_all))   # 对数据进行标准化    #img = io.imread("D:\yunding\paper_four\HSI_data\Indian_29_42_89.png")    data_seg = slic(data_all, n_segments=2000, compactness=0.1) # indian slic 分割 1500, 0.01; pavia 分割 1500, 1    #out = mark_boundaries(img, data_seg)    #plt.imshow(out)    #plt.show()    #sio.savemat('data_seg.mat', {'data_seg': data_seg})    print('this is data shape', data_all.shape)        row_gnd = np.size(gnd, 0)    col_gnd = np.size(gnd, 1)    dim = np.size(data_all, 2)    gnd = np.reshape(gnd, (1, row_gnd * col_gnd)) # ground truth    ground_truth = gnd.copy()    fea = np.reshape(data_all, (1, row_gnd * col_gnd, dim))    fea = np.squeeze(fea)    #kmeans = KMeans(n_clusters=100, random_state=0).fit(fea)    #data_seg = kmeans.labels_    gnd = np.squeeze(gnd)    ground_truth = gnd.copy()    choice = np.where(gnd != 0)      segments = np.reshape(data_seg, (1, row_gnd * col_gnd))    segments = np.squeeze(segments)    num_class = gnd.max()    num_class = num_class.astype(int)    index_all_block = []      num_Per_class = np.zeros((1, num_class))    """ idx_train idx_test """    #training = [30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30] #indian %     #training = [30,30,30,30,30,30,15,30,15,30,30,30,30,30,30,30]       #training = [40, 40, 40, 40, 40, 40, 15, 40, 15, 40, 40, 40, 40, 40, 40, 40]  # indian    #training = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]    training = [3, 4, 2, 2, 3, 4, 4, 12, 7, 4, 2, 2, 1, 2, 8, 2]    # training = [7, 19, 3, 4, 2, 6, 2, 4, 1]    #training = [30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]        training = np.array(training)    per_class_train_num = np.zeros((1, num_class))    idx_train_index = []    idx_test_index = []    for i in range(num_class):        ii = i + 1.        a2 = np.where(gnd == ii)        a2 = np.array(a2)        a2 = np.squeeze(a2)        num_Per_class[0, np.array(ii - 1.).astype(int)] = len(a2)        index_all_block = np.concatenate((index_all_block, a2))        #idx_train_1 = random.sample(list(a2), np.floor(0.1 * len(a2)).astype(int))        #idx_train_1 = random.sample(list(a2), per_class_sample)        idx_train_1 = random.sample(list(a2), training[i])        idx_train_index = np.concatenate((idx_train_index, idx_train_1))        per_class_train_num[0, np.array(ii - 1.).astype(int)] = len(idx_train_index)        #pdb.set_trace()         idx_test_1 = np.setdiff1d(a2, idx_train_index, True)        idx_test_index = np.concatenate((idx_test_index, idx_test_1[0:np.ceil(len(idx_test_1)).astype(int)]))    index_all = np.concatenate((idx_train_index.astype(int), idx_test_index.astype(int)))    fea_train = fea[idx_train_index.astype(int),:]    fea_test = fea[idx_test_index.astype(int),:]    fts = np.concatenate((fea_train, fea_test))  # fts = samples*dimension    lbls = np.concatenate((gnd[idx_train_index.astype(int)],gnd[idx_test_index.astype(int)])) #find labels    seg_labels = np.concatenate((segments[idx_train_index.astype(int)], segments[idx_test_index.astype(int)]))    unique_seg_lbls = np.unique(seg_labels)    len_seg_lbls = len(unique_seg_lbls) # find the unique length of segment labels    mean_pixel = np.zeros((1, dim))    unique_seg_labels = np.unique(seg_labels)    max_seg_lab = np.max(segments)    YY = np.zeros((fea_train.shape[0],lbls.max().astype(int)))    index_seg_all = []    for j in range(len_seg_lbls):        index_seg = np.where(seg_labels == unique_seg_labels[j])                index_seg = np.column_stack(index_seg)        bb = len(index_seg)        cc = np.sum(fts[index_seg,:], axis=0)/bb        mean_pixel = np.concatenate((mean_pixel, cc))    mean_pixel = np.delete(mean_pixel,0, axis=0)     dist1 = compute_dist(np.array(fts), np.array(mean_pixel))    dist_mean1 = compute_dist(np.array(mean_pixel), np.array(mean_pixel))    spatial_coordinates = sptial_neighbor_matrix(index_all, gt)  #    mean_coordinates = np.zeros((1, spatial_coordinates.shape[1]))#max_seg_lab    for jj in range(len_seg_lbls):        index_seg = np.where(seg_labels == unique_seg_labels[jj])  #                 index_seg = np.column_stack(index_seg)        bb = len(index_seg)        cc = np.sum(spatial_coordinates[index_seg,:], axis=0)/bb        mean_coordinates = np.concatenate((mean_coordinates, cc))    mean_coordinates = np.delete(mean_coordinates,0, axis=0) #    dist3 = compute_dist(np.array(spatial_coordinates), np.array(mean_coordinates))    dist3 = dist3 / np.tile(np.sqrt(np.sum(dist3 ** 2, 1))[..., np.newaxis], (1,dist3.shape[1]))    dist_mean3 = compute_dist(np.array(mean_coordinates), np.array(mean_coordinates))    dist_mean3 = dist_mean3 / np.tile(np.sqrt(np.sum(dist_mean3 ** 2, 1))[..., np.newaxis], (1,dist_mean3.shape[1]))    dist = dist1+50*dist3#30  +parameter[r]    lam = 15  #parameter[r1]  #indian 10  pavia 5    S = np.zeros((np.array(fts).shape[0], len_seg_lbls))    for k in range(np.array(fts).shape[0]):        idxa0 = range(len_seg_lbls)        di = dist[k,:]        ad = -0.5 * lam * di        S[k][idxa0] = EProjSimplex_new(ad)    dist_mean = dist_mean1+50 *dist_mean3    beta = 15  #indian :12 pavia 5 lam1    S_mean= np.zeros((len_seg_lbls, len_seg_lbls))    for k in range(len_seg_lbls):        idxa0 = range(len_seg_lbls)        di = dist_mean[k,:]        ad = -0.5 * beta * di        S_mean[k][idxa0] = EProjSimplex_new(ad)    S_mean = S_mean - np.diag(np.diag(S_mean))    S_mean = (S_mean + S_mean.transpose())/2    #DS = np.sum(S_mean, axis=1)    DS = np.mat(np.diag(np.sum(S_mean, axis=1)))    L_S_mean = DS - S_mean    S_mean_opt = (np.eye(len_seg_lbls)+L_S_mean/0.1).I  #0.1    S_mean_opt = np.array(S_mean_opt)    S = np.dot(S,S_mean_opt)    eps = 1e-5    DE = np.sum(S, axis=0)    invDE = np.mat(np.diag(np.power(DE+eps, -1)))    #pdb.set_trace() ######################################################################    #########################################################################################                            #########################################################################################    G = np.dot(S, invDE).dot(S.transpose())    lbls = lbls - 1    #lbls = lbls.astype(np.long)    idx = range(len(lbls))    idx_train = idx[0:len(idx_train_index)]    idx_test = idx[len(idx_train_index):]    return fts, lbls, idx_train_index, idx_train, idx_test_index, idx_test, G, mean_pixel, index_all, ground_truth# if torch.max(mx) == torch.min(mx):#     return np.ones_like(mx, dtype=np.float32) * 1e-6# else:#     return 1.0 * (mx - torch.min(mx)) / (torch.max(mx) - torch.min(mx))# return 1.0 * (mx - np.min(mx)) / (np.max(mx) - np.min(mx))def normalize(nparray, order=2, axis=0):    """Normalize a N-D numpy array along the specified axis."""    nparray = np.array(nparray)    norm = np.linalg.norm(nparray, ord=order, axis=axis, keepdims=True)    return nparray / (norm + np.finfo(np.float32).eps)def accuracy(output, labels):    preds = output.max(1)[1].type_as(labels)    correct = preds.eq(labels).double()    correct = correct.sum()    return correct / len(labels)def sparse_mx_to_torch_sparse_tensor(sparse_mx):    """Convert a scipy sparse matrix to a torch sparse tensor."""    sparse_mx = sparse_mx.tocoo().astype(np.float32)    indices = torch.from_numpy(        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))    values = torch.from_numpy(sparse_mx.data)    shape = torch.Size(sparse_mx.shape)    return torch.sparse.FloatTensor(indices, values, shape)def sptial_neighbor_matrix(index_all, gt):    """ extract the spatial coordinates(x,y) """    L_cor = np.zeros([2, 1])    for kkk in range(len(index_all)):        [X_cor, Y_cor] = ind2sub([np.size(gt, 0), np.size(gt, 1)], index_all[kkk])        XY_cor = np.array([X_cor, Y_cor])[..., np.newaxis]        L_cor = np.concatenate((L_cor, XY_cor), axis=1)    L_cor = np.delete(L_cor, 0, axis=1)  # delete 0 column    return L_cor.transpose()def compute_dist(array1, array2, type='euclidean'):    """Compute the euclidean or cosine distance of all pairs.  Args:    array1: numpy array with shape [m1, n]    array2: numpy array with shape [m2, n]    type: one of ['cosine', 'euclidean']  Returns:    numpy array with shape [m1, m2]  """    assert type in ['cosine', 'euclidean']    if type == 'cosine':        #array1 = normalize(array1, axis=1)       # array2 = normalize(array2, axis=1)        dist = np.matmul(array1, array2.T)        return dist    else:        # shape [m1, 1]        square1 = np.sum(np.square(array1), axis=1)[..., np.newaxis]        # shape [1, m2]        square2 = np.sum(np.square(array2), axis=1)[np.newaxis, ...]        squared_dist = - 2 * np.matmul(array1, array2.T) + square1 + square2        squared_dist[squared_dist < 0] = 0        dist = np.sqrt(squared_dist)#np.exp(dist - np.tile(np.max(dist, axis=0)[..., np.newaxis], np.size(dist, 1)))        return dist